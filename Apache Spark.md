Apache Spark - это фреймворк для распределенных вычислений и обработки данных.  Spark предоставляет компонуемые и согласованные API с помощью которых вы можете строить приложение и расширять его своими библиотеками. 
Spark обеспечивает высокую производительность за счет оптимизации разных библиотек. Он может эффективно обработать SQL запрос и сделать оценку ML-модели за один проход.
Spark унифицированная платформа, которая позволяет использовать только один фреймворк с большим набором библиотек. До Spark небыло подобных систем и пользователям приходилось работать с большим зоопарком технологий.
Apache Spark поддерживает языки программирования SQL, R, Python, Scala и Java. 
Фреймворк может запускаться как на локальном ноутбуке, так и на больших удаленных 
кластерах.

Тут изображены все компоненты [[Spark’s toolkit]]

Spark ограничивает свою область в роли вычислительного движка, он отвечает за загрузку данных из систем хранения и вычисление их, но не отвечает за долговременное хранение.

[[Библиотеки Spark]]
[[Проблема больших данных]]
[[Менеджеры кластеров]]
[[Spark приложение]]
[[spark-submit]]

#Spark #Dataengineering
