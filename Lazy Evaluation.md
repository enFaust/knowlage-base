Ленивые вычисления - Spark ждет до последнего момента, чтобы начать выполнять граф вычислительных инструкций. Вы наращиваете план преобразований, который хотите применить к данным, вместо того, чтобы начать выполнять его сразу.
Это позволяет оптимизировать весь конвеер данных до его запуска.

Примером такой оптимизации является [[predicate pushdown]] для [[DataFrame]]. При построении большого SparkJob мы можем указать в самом конце фильтр. который требует вытащить всего 1 строку. Spark оптимизирует это и обратится только к нужной записи, проталкивая фильтр вниз по плану.

